{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 20:17:28 WARN Utils: Your hostname, willb-boxx resolves to a loopback address: 127.0.1.1; using 192.168.7.214 instead (on interface wlo2)\n",
      "21/10/14 20:17:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/willb/.local/share/virtualenvs/fraud-notebooks-eQN7wzh0/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/14 20:17:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/14 20:17:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import time, timeit\n",
    "output_ts = int(time.time())\n",
    "\n",
    "df = session.read.parquet(\"fraud-all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "These data are mostly clean but we need to add a new field for transaction interarrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- merchant_id: long (nullable = true)\n",
      " |-- trans_type: string (nullable = true)\n",
      " |-- foreign: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.window as W\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "interarrival_spec = W.Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "overall_spec = W.Window.orderBy(\"timestamp\")\n",
    "\n",
    "df_interarrival = df.withColumn(\n",
    "    \"previous_timestamp\", \n",
    "    F.lag(df[\"timestamp\"]).over(\n",
    "        interarrival_spec\n",
    "    )\n",
    ").withColumn(\n",
    "    \"interarrival\",\n",
    "    (F.col(\"timestamp\") - F.col(\"previous_timestamp\")).cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    split_point = int(df_interarrival.count() * 0.7)\n",
    "\n",
    "    df_interarrival_split = df_interarrival.withColumn(\n",
    "        \"amount_rank_user_rolling\",\n",
    "        (F.rank().over(rollingUserSpec) / \n",
    "         F.count(\"user_id\").over(rollingUserSpec)).cast(\"float\") \n",
    "    ).withColumn(\"transactions_in_rolling_window\",\n",
    "        F.count(\"user_id\").over(rollingUserSpec)   \n",
    "    )\n",
    "    \n",
    "    df_interarrival_train = df_interarrival_split.where(F.col(\"observation_number\") <= split_point)\n",
    "    df_interarrival_test = df_interarrival_split.where(F.col(\"observation_number\") > split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# never computed; an option for comparison\n",
    "\n",
    "if False:\n",
    "    df_dist_unused = df_interarrival_train.\\\n",
    "        withColumn(\"amount_quantile\",\n",
    "            F.cume_dist().over(\n",
    "                W.Window.partitionBy(\"user_id\").orderBy(\"amount\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.conf.set(\"spark.rapids.sql.castFloatToIntegralTypes.enabled\", True)\n",
    "\n",
    "amount_cents = (F.col(\"amount\") * 100).cast(\"int\")\n",
    "\n",
    "rollingUserSpec = \\\n",
    "    W.Window.partitionBy(\"user_id\").orderBy(\n",
    "        F.col(\"timestamp\")\n",
    "    ).rangeBetween(\n",
    "        -(60 * 60 * 24 * 7),\n",
    "        W.Window.currentRow\n",
    "    ).orderBy(\n",
    "        amount_cents\n",
    "    )\n",
    "\n",
    "\n",
    "userSpec = \\\n",
    "    W.Window.partitionBy(\"user_id\").orderBy(\n",
    "        amount_cents\n",
    "    )\n",
    "\n",
    "toPresentUserSpec = \\\n",
    "    W.Window.partitionBy(\"user_id\").orderBy(\n",
    "        F.col(\"timestamp\")\n",
    "    ).rowsBetween(\n",
    "        W.Window.unboundedPreceding,\n",
    "        W.Window.currentRow\n",
    "    ).orderBy(\n",
    "        amount_cents\n",
    "    )\n",
    "\n",
    "overallSpec = \\\n",
    "    W.Window.orderBy(\n",
    "        amount_cents\n",
    "    )\n",
    "\n",
    "# not identical to cume_dist; this rank is the fraction of \n",
    "# transactions that are strictly less than the current row\n",
    "\n",
    "# XXX:  need to censor overall and user-overall quantiles with train/test split\n",
    "\n",
    "df_dist = df_interarrival.\\\n",
    "    withColumn(\"amount_rank_user\",\n",
    "        (F.rank().over(userSpec) / \n",
    "         F.count(\"user_id\").over(userSpec)).cast(\"float\")\n",
    "    ).withColumn(\"amount_rank_overall\",\n",
    "         (F.rank().over(overallSpec) / \n",
    "         F.count(\"user_id\").over(overallSpec)).cast(\"float\")       \n",
    "    ).withColumn(\"amount_rank_user_to_present\",\n",
    "        (F.rank().over(toPresentUserSpec) / \n",
    "         F.count(\"user_id\").over(toPresentUserSpec)).cast(\"float\") \n",
    "    ).withColumn(\"user_transaction_count_to_present\",\n",
    "         F.count(\"user_id\").over(toPresentUserSpec)   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- merchant_id: long (nullable = true)\n",
      " |-- trans_type: string (nullable = true)\n",
      " |-- foreign: boolean (nullable = true)\n",
      " |-- previous_timestamp: long (nullable = true)\n",
      " |-- interarrival: integer (nullable = true)\n",
      " |-- amount_rank_user: float (nullable = true)\n",
      " |-- amount_rank_overall: float (nullable = true)\n",
      " |-- amount_rank_user_to_present: float (nullable = true)\n",
      " |-- user_transaction_count_to_present: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dist.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_dist.drop(\n",
    "    \"previous_timestamp\"\n",
    ").withColumn(\n",
    "    \"amount\", \n",
    "    F.col(\"amount\").cast(\"float\")\n",
    ").withColumn(\n",
    "    \"user_id\", \n",
    "    F.col(\"user_id\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"merchant_id\", \n",
    "    F.col(\"merchant_id\").cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interarrival_calc = timeit.timeit(lambda: df_interarrival.write.parquet(f\"fraud-interarrival-{output_ts}.parquet\"), number=1)\n",
    "quantile_calc = timeit.timeit(lambda: df_out.write.parquet(f\"fraud-cleaned-{output_ts}.parquet\"), number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.sample(fraction=0.05).write.parquet(f\"fraud-cleaned-{output_ts}-sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"time to compute interarrivals:  {interarrival_calc}\")\n",
    "print(f\"time to compute quantiles:  {quantile_calc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
